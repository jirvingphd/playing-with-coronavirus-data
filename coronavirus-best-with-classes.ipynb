{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Coronavirus Timeseries\n",
    "\n",
    "- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stupid edeit\n",
    "- This notebook uses 2 classes (based on a BaseDataset class) to load in data from both a kaggle dataset (novel coronavirus 2019) and the Covid Tracking Project data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Add data from Covid Tracking Project's API\n",
    "    - https://covidtracking.com/api\n",
    "    \n",
    "- [ ] Move app styling to a css file in a new `assets/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functions and classes are in functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESOURCES FOR FUTURE\n",
    "- RAFAEL STUDY GROUP FOR MAKING A MAP\n",
    "    - https://www.youtube.com/watch?v=MAhK7NHXEOg&feature=emb_logo\n",
    "    - https://github.com/erdosn/additional-topic-plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.151805Z",
     "start_time": "2020-07-04T01:16:05.310232Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/anaconda3/envs/learn-env/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "/anaconda3/envs/learn-env/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds v0.2.15 loaded.  Read the docs: https://fs-ds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row1_col1\" class=\"data row1 col1\" >fsds</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_f325f7e8_bd93_11ea_a4c0_4865ee12e626row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1070369b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Pandas .iplot() method activated.\n"
     ]
    }
   ],
   "source": [
    "import os,glob,sys\n",
    "import re\n",
    "\n",
    "!pip install -U fsds\n",
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Kaggle Dataset - Get US States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def  `download_coronavirus_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.155001Z",
     "start_time": "2020-07-04T01:16:14.153051Z"
    }
   },
   "outputs": [],
   "source": [
    "# import kaggle.api as kaggle\n",
    "# kaggle.authenticate()\n",
    "# help(kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.158575Z",
     "start_time": "2020-07-04T01:16:14.156590Z"
    }
   },
   "outputs": [],
   "source": [
    "# kaggle datasets download -d sudalairajkumar/novel-corona-virus-2019-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.161765Z",
     "start_time": "2020-07-04T01:16:14.160033Z"
    }
   },
   "outputs": [],
   "source": [
    "# kaggle.dataset_download_files('sudalairajkumar/novel-corona-virus-2019-dataset',\n",
    "#                              path='New Data/',force=True,unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.165383Z",
     "start_time": "2020-07-04T01:16:14.162961Z"
    }
   },
   "outputs": [],
   "source": [
    "# # @add_method(CoronaData)\n",
    "# def download_coronavirus_data(path='New Data/',verbose=False):\n",
    "#     \"\"\"Installs the Kaggle Command Line Interface to clone dataset.\n",
    "#     Then extracts dataset to specified path and displays name of main file.\n",
    "#     Args:\n",
    "#         path(str): Folder to extract dataset into (must end with a '/')\n",
    "        \n",
    "#     Returns:\n",
    "#         file_list(list): List of full filepaths to downloaded csv files.\n",
    "#     \"\"\"\n",
    "#     ## Determine if dataset is downloaded via Kaggle CL\n",
    "#     import os,glob\n",
    "#     from zipfile import ZipFile\n",
    "#     from IPython.display import clear_output\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "\n",
    "#     ## Install Kaggle \n",
    "#     !pip install kaggle --upgrade\n",
    "#     clear_output()\n",
    "    \n",
    "#     ## Run Kaggle Command \n",
    "#     cmd = 'kaggle datasets download -d sudalairajkumar/novel-corona-virus-2019-dataset'\n",
    "#     os.system(cmd)\n",
    "    \n",
    "#     ## Extract ZipFile\n",
    "#     print(f'Downloaded dataset Zipfie, extracting to {path}...')\n",
    "#     zip_filepath = 'novel-corona-virus-2019-dataset.zip'\n",
    "#     with ZipFile(zip_filepath) as file:\n",
    "#         file.extractall(path)\n",
    "    \n",
    "#     ## Delete Zip File\n",
    "#     os.system(f\"rm {zip_filepath}\"  )\n",
    "    \n",
    "    \n",
    "#     ## Get list of all csvs\n",
    "#     print('[i] Extraction Complete.')    \n",
    "#     file_list = glob.glob(path+\"*.csv\")\n",
    "    \n",
    "    \n",
    "#     ## Find main df \n",
    "#     main_file = [file for file in file_list if 'covid_19_data.csv' in file]\n",
    "#     if verbose:\n",
    "#         print(f\"[i] The main file name is {main_file}\")\n",
    "#     return main_file[0] #file_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.168370Z",
     "start_time": "2020-07-04T01:16:14.166553Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DOWNLOAD = True\n",
    "\n",
    "# if DOWNLOAD:\n",
    "#     main_file = download_coronavirus_data()\n",
    "#     print('\\n\\n[i] Success. Downloaded dataset from kaggle...')\n",
    "\n",
    "# else:\n",
    "#     print('Using pre-existing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¦class `CoronaData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.173139Z",
     "start_time": "2020-07-04T01:16:14.169522Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_method(cls):\n",
    "    from functools import wraps # This convenience func preserves name and docstring\n",
    "\n",
    "    \"\"\"source=https://medium.com/@mgarod/dynamically-add-a-method-to-a-class-in-python-c49204b85bd6\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func) \n",
    "        def wrapper(self, *args, **kwargs): \n",
    "            return func(*args, **kwargs)\n",
    "        setattr(cls, func.__name__, wrapper)\n",
    "        # Note we are not binding func, but wrapper which accepts self but does exactly the same as func\n",
    "        return func # returning func means func can still be used normally\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.188769Z",
     "start_time": "2020-07-04T01:16:14.174969Z"
    }
   },
   "outputs": [],
   "source": [
    "#Make a base class\n",
    "class BaselineData(object):\n",
    "    import pandas as pd\n",
    "    _df = pd.DataFrame()\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        if hasattr(self,'_df_type'):\n",
    "            print(self._df_type)\n",
    "        if hasattr(self,'_df'):\n",
    "            return self._df.copy()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "    @df.setter \n",
    "    def df(self,value):\n",
    "        self._df = value\n",
    "        \n",
    "        \n",
    "    def get_group_ts(self,group_name,group_col='state',\n",
    "                     ts_col=None,df=None,\n",
    "                     freq='D', agg_func='sum'):\n",
    "        \"\"\"Take df_us and extracts state's data as then Freq/Aggregation provided\"\"\"\n",
    "        ## \n",
    "        if df is None:\n",
    "            df = self._df.copy()\n",
    "            \n",
    "        try:\n",
    "            ## Get state_df group\n",
    "            group_df = df.groupby(group_col).get_group(group_name)#.resample(freq).agg(agg)\n",
    "        except Exception:\n",
    "            display(df.head())\n",
    "            return None\n",
    "        ## Resample and aggregate state data\n",
    "        group_df = group_df.resample(freq).agg(agg_func)\n",
    "\n",
    "\n",
    "        ## Get and Rename Sum Cols \n",
    "        orig_cols = group_df.columns\n",
    "\n",
    "        ## Create Renamed Sum columns\n",
    "        for col in orig_cols:\n",
    "            group_df[f\"{group_name} - {col}\"] = group_df[col]\n",
    "\n",
    "        ## Drop original cols\n",
    "        group_df.drop(orig_cols,axis=1,inplace=True)\n",
    "\n",
    "        if ts_col is not None:\n",
    "            ts_cols_selected = [col for col in group_df.columns if ts_col in col]\n",
    "            group_df = group_df[ts_cols_selected]\n",
    "\n",
    "        return group_df \n",
    "        \n",
    "\n",
    "    ### CLASS DISPLAY RELATED ITEMS\n",
    "    def _self_report(self,private=False,\n",
    "                     methods=True,attributes=True,\n",
    "                    workflow=False):\n",
    "        import inspect\n",
    "        attr_list = inspect.getmembers(self)\n",
    "        dashes='---'*20\n",
    "        report = [dashes]\n",
    "        report.append(\"[i] CovidTrackingProject Contents:\\n\"+dashes)\n",
    "\n",
    "\n",
    "        method_list=[\"\\nMETHODS:\"]\n",
    "        attribute_list=[\"\\nATTRIBUTES\"]\n",
    "        workflow_list = [\"\\nWORKFLOW:\"]\n",
    "        \n",
    "        if private==False:\n",
    "            startswithcheck = '_'\n",
    "        else:\n",
    "            startswithcheck ='__'\n",
    "        \n",
    "        ## Loop through all attr\n",
    "        for item in attr_list:\n",
    "            item_name = item[0]\n",
    "            \n",
    "            ## Exclude Private/Special Attrs\n",
    "            if item_name.startswith(startswithcheck)== False:\n",
    "                \n",
    "                ## Get tf if item is method\n",
    "                method_check = inspect.ismethod(item[1])\n",
    "                \n",
    "                ## If item is a method:\n",
    "                if method_check==True:\n",
    "                    method_list.append(item_name)\n",
    "                ## If item is an attribute\n",
    "                else: \n",
    "                    attribute_list.append(item_name) \n",
    "                    \n",
    "        if workflow:\n",
    "            ## Get workflow\n",
    "            workflow_funcs = [self.download_coronavirus_data,\n",
    "                             self.load_raw_df, self.get_and_clean_US]\n",
    "            for i,method in enumerate(workflow_funcs):\n",
    "                workflow_list.append(f\"{i+1}. {method.__name__}\")\n",
    "\n",
    "            report.append('\\n\\t'.join(workflow_list))\n",
    "        \n",
    "        if methods:\n",
    "            report.append('\\n\\t'.join(method_list))\n",
    "        if attributes:\n",
    "            report.append('\\n\\t'.join(attribute_list))\n",
    "            \n",
    "        return '\\n'.join(report)\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return self._self_report()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._self_report()      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.196481Z",
     "start_time": "2020-07-04T01:16:14.190144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = BaselineData()\n",
    "base.df \n",
    "# get_group_ts('ny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.974604Z",
     "start_time": "2020-07-04T01:16:14.197743Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functions import CoronaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:14.984554Z",
     "start_time": "2020-07-04T01:16:14.975865Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# class CoronaData(BaselineData):\n",
    "\n",
    "#     def __init__(self,data_dir='New Data/',run_workflow=True,\n",
    "#                  download=True,verbose=True):\n",
    "        \n",
    "#         ## Save params for later\n",
    "#         self.__download = download\n",
    "#         self.__verbose = verbose\n",
    "#         self._data_folder = data_dir\n",
    "        \n",
    "#         ## Download data or set local filepath\n",
    "#         if download:\n",
    "# #             print(\"[i] DOWNLOADING DATA FROM KAGGLE:\")\n",
    "#             self.download_coronavirus_data(verbose=verbose)\n",
    "            \n",
    "#         else:\n",
    "#             self.get_data_fpath(data_dir)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         ## Load df_raw and df\n",
    "#         self.load_raw_df(verbose=verbose)\n",
    "        \n",
    "#         ## Prepare State Data\n",
    "#         if run_workflow:\n",
    "#             self.get_and_clean_US()\n",
    "#             self._make_state_dict()\n",
    "# #             print('\\n[!] Full Worfklow Complete:')\n",
    "# #             print('\\tself.STATES, self.df_us created.')\n",
    "            \n",
    "\n",
    "#     # @add_method(CoronaData)\n",
    "#     def download_coronavirus_data(self,path=None,verbose=None):\n",
    "#         \"\"\"Installs the Kaggle Command Line Interface to clone dataset.\n",
    "#         Then extracts dataset to specified path and displays name of main file.\n",
    "#         Args:\n",
    "#             path(str): Folder to extract dataset into (must end with a '/')\n",
    "\n",
    "#         Returns:\n",
    "#             file_list(list): List of full filepaths to downloaded csv files.\n",
    "#         \"\"\"        \n",
    "#         if verbose==None:\n",
    "#             verbose = self.__verbose\n",
    "            \n",
    "#         if verbose:\n",
    "#             print('[i] DOWNLOADING DATA USING KAGGLE API')\n",
    "#             print(\"\\thttps://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\")\n",
    "\n",
    "#         if path is None:\n",
    "#             path = self._data_folder\n",
    "                                  \n",
    "#         ## Determine if dataset is downloaded via Kaggle CL\n",
    "#         import os,glob\n",
    "#         from zipfile import ZipFile\n",
    "#         from IPython.display import clear_output\n",
    "#         os.makedirs(path, exist_ok=True)\n",
    "\n",
    "#         try:\n",
    "#             import kaggle\n",
    "#         except:\n",
    "#             ## Install Kaggle \n",
    "#             !pip install kaggle --upgrade\n",
    "#             clear_output()\n",
    "#             if verbose: print('\\t- Installed kaggle command line tool.')\n",
    "\n",
    "#         ## Run Kaggle Command \n",
    "#         cmd = 'kaggle datasets download -d sudalairajkumar/novel-corona-virus-2019-dataset'\n",
    "#         os.system(cmd)\n",
    "\n",
    "#         ## Extract ZipFile\n",
    "#         zip_filepath = 'novel-corona-virus-2019-dataset.zip'\n",
    "#         with ZipFile(zip_filepath) as file:\n",
    "#             file.extractall(path)\n",
    "            \n",
    "#         if self.__verbose:\n",
    "#             print(f'\\t- Downloaded dataset .zip and extracted to:\"{path}\"')\n",
    "     \n",
    "#         ## Delete Zip File\n",
    "#         os.system(f\"rm {zip_filepath}\"  )\n",
    "            \n",
    "#         self.get_data_fpath(path)\n",
    "\n",
    "        \n",
    "#     def get_data_fpath(self,path):\n",
    "#         \"\"\"save self._file_list and self._main_file\"\"\"\n",
    "#         import glob\n",
    "#         verbose = self.__verbose\n",
    "#         ## Get list of all csvs\n",
    "#         if verbose: print('\\t- Extraction Complete.')    \n",
    "#         file_list = glob.glob(path+\"*.csv\")\n",
    "\n",
    "#         ## Find main df \n",
    "#         main_file = [file for file in file_list if 'covid_19_data.csv' in file]\n",
    "# #         if verbose: print(f\"- The main file name is {main_file}\")\n",
    "#         self._file_list = file_list\n",
    "#         self._main_file = main_file[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def load_raw_df(self,fpath=None,kws={},verbose=True):\n",
    "#         \"\"\"Performs most basic of preprocessing, including renaming date column to \n",
    "#         Date and dropping 'Last Update', and 'SNo' columns\"\"\"\n",
    "#         import pandas as pd\n",
    "#         if fpath is None:\n",
    "#             fpath = self._main_file\n",
    "\n",
    "#         ## Default Kws\n",
    "#         read_kws = dict(parse_dates=['ObservationDate','Last Update'])\n",
    "\n",
    "#         ## Add User kws\n",
    "#         read_kws = {**read_kws,**kws}\n",
    "\n",
    "# #         if verbose:\n",
    "# #             print(f\"[i] Loading {fpath} with read_csv kws:\",end='')\n",
    "# #             display(read_kws)\n",
    "\n",
    "#         ## Read in csv and save as self.df_raw\n",
    "#         df = pd.read_csv(fpath,**read_kws)\n",
    "#         self.df_raw = df.copy()\n",
    "#         ## Drop unwated columns\n",
    "#         df.drop(['Last Update',\n",
    "#                  'SNo'],axis=1,inplace=True)\n",
    "        \n",
    "\n",
    "#         ## Rename Date columns\n",
    "#         df.rename({'ObservationDate':'Date'},axis=1,inplace=True)\n",
    "\n",
    "#         ## Display some info \n",
    "#         if verbose:\n",
    "#             display(df.head())\n",
    "#             # Countries in the dataset\n",
    "#             print(f\"[i] There are \"+str(len(df['Country/Region'].unique()))+\" countries in the datatset\")\n",
    "\n",
    "#             ## Get first and last date\n",
    "#             start_ts = df[\"Date\"].loc[df['Date'].idxmin()].strftime('%m-%d-%Y')\n",
    "#             end_ts = df[\"Date\"].loc[df['Date'].idxmax()].strftime('%m-%d-%Y')\n",
    "#             # DF['Date'].idxmin(), DF['Date'].idxmax()\n",
    "#             print(f\"[i] Dates Covered:\\n\\tFrom {start_ts} to {end_ts}\")\n",
    "\n",
    "#         self._df = df.copy()#self.set_datetime_index(df)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "#     def set_datetime_index(self,df_=None,col='Date'):#,drop_old=False):\n",
    "#         \"\"\"Returns df with specified column as datetime index\"\"\"\n",
    "#         import pandas as pd\n",
    "\n",
    "#         ## Grab df from self if None\n",
    "#         if df_ is None:\n",
    "#             df_ = self.df\n",
    "            \n",
    "#         ## Copy to avoid edits to orig\n",
    "#         df = df_.copy()\n",
    "        \n",
    "#         ## Convert to date time\n",
    "#         df[col] = pd.to_datetime(df[col],infer_datetime_format=True)\n",
    "        \n",
    "#         ## Set as index\n",
    "#         df.set_index(df[col],drop=True,inplace=True)\n",
    "        \n",
    "#         # Drop the column if it is present\n",
    "#         if col in df.columns:\n",
    "#             df.drop(columns=col,inplace=True)\n",
    "            \n",
    "#         return df\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def load_us_reference_info(self):\n",
    "#         \"\"\"Return and save US Reference Data\"\"\"\n",
    "#         ## Making Master Lookup CSV\n",
    "#         import pandas as pd\n",
    "#         abbrev = pd.read_csv('Reference Data/united_states_abbreviations.csv')\n",
    "#         pop = pd.read_csv('Reference Data/us-pop-est2019-alldata.csv')\n",
    "#         us_pop = pop.loc[pop['STATE']>0][['NAME','POPESTIMATE2019']].copy()\n",
    "#         us_info = pd.merge(abbrev,us_pop,right_on='NAME',left_on='State',how=\"inner\")\n",
    "#         us_info.drop('NAME',axis=1,inplace=True)\n",
    "#         self.reference_data = us_info\n",
    "#         return us_info\n",
    "    \n",
    "    \n",
    "#     def calculate_per_capita(self,df_=None,stat_cols = ['Confirmed','Deaths','Recovered']):\n",
    "#         \"\"\"Calculate Per Capita columns\"\"\"\n",
    "#         if df_ is None:\n",
    "#             df_ = self.df\n",
    "            \n",
    "#         df = df_.copy()\n",
    "        \n",
    "#         if 'POPESTIMATE2019' in df.columns==False:\n",
    "#             self.load_us_reference_info()\n",
    "            \n",
    "#         ## ADDING PER CAPITA DATA \n",
    "#         for col in stat_cols:\n",
    "#             df[f\"{col} Per Capita\"] = df[col]/df['POPESTIMATE2019']\n",
    "#         df.drop('POPESTIMATE2019',axis=1,inplace=True)\n",
    "#         return df    \n",
    "\n",
    "    \n",
    "    \n",
    "#     def get_and_clean_US(self,df=None,#save_as = 'Reference Data/united_states_abbreviations.csv',\n",
    "#                          make_date_index=True,per_capita=True):\n",
    "#         \"\"\"Takes raw df loaded and extracts United States and processes\n",
    "#         all state names to create new abbreviation column 'state'.\n",
    "#         \"\"\"\n",
    "#         import pandas as pd\n",
    "#         if df is None:\n",
    "#             df= self._df.copy()\n",
    "            \n",
    "#         ## Get only US\n",
    "#         df_us = df.groupby('Country/Region').get_group('US').copy() \n",
    "#         state_lookup = self.load_us_reference_info()\n",
    "\n",
    "\n",
    "#         ## Make renaming dict for states\n",
    "#         STATE_DICT = dict(zip(state_lookup['State'],state_lookup['Abbreviation']))\n",
    "#         STATE_DICT.update({'Chicago':'IL',\n",
    "#                           'Puerto Rico':'PR',\n",
    "#                           'Virgin Islands':'VI',\n",
    "#                           'United States Virgin Islands':'VI'})\n",
    "\n",
    "#         ## Separately Process Rows that contain a city, state \n",
    "#         df_city_states = df_us[df_us['Province/State'].str.contains(',')]\n",
    "\n",
    "\n",
    "#         ## Finding City Abbreviations in city_states\n",
    "#         import re\n",
    "#         state_expr = re.compile(r\"[A-Z\\.]{2,4}\")\n",
    "#         df_city_states['state'] = df_city_states['Province/State'].apply(state_expr.findall)\n",
    "#         df_city_states = df_city_states.explode('state')\n",
    "\n",
    "\n",
    "#         ## Seperately process Rows that do not contain a city,state\n",
    "#         df_states = df_us[~df_us['Province/State'].str.contains(',')]\n",
    "#         df_states['state'] =  df_states['Province/State'].map(STATE_DICT)\n",
    "\n",
    "#         ## Combining data frame back together\n",
    "#         df = pd.concat([df_states,df_city_states]).sort_index()\n",
    "# #         df = df.dropna(subset=['state'])\n",
    "\n",
    "#         ## Fix some stragglers (like D.C. vs DC)\n",
    "#         df['state'] = df['state'].replace('D.C.','DC')\n",
    "        \n",
    "#         ## Combine Cleaned Data \n",
    "#         df = pd.merge(df, state_lookup,left_on='state',right_on=\"Abbreviation\")\n",
    "        \n",
    "#         df.rename({'State':'State Name'},inplace=True,axis=1)\n",
    "#         df.drop(columns=['Abbreviation','State Name'],inplace =True)\n",
    "        \n",
    "    \n",
    "#         ## Add Population Data\n",
    "#         if per_capita:\n",
    "\n",
    "#             for col in  ['Confirmed','Deaths','Recovered']:\n",
    "#                 df[f\"{col} Per Capita\"] = df[col]/df['POPESTIMATE2019']\n",
    "\n",
    "#             ## Remove Population \n",
    "#             df.drop('POPESTIMATE2019',axis=1,inplace=True)\n",
    "\n",
    "#         if make_date_index:\n",
    "#             df = self.set_datetime_index(df)\n",
    "        \n",
    "# #         df.drop(columns=['Province/State'],inplace=True)\n",
    "\n",
    "#         self.df_us = df.copy()\n",
    "# #         self.US = df.copy()\n",
    "#         return df\n",
    "    \n",
    "    \n",
    "# #     def get_state_ts(self,state_name,df=None,\n",
    "# #                      group_col='state', ts_col=None,\n",
    "# #                      freq='D', agg_func='sum'):\n",
    "# #         \"\"\"Take df_us and extracts state's data as then Freq/Aggregation provided\"\"\"\n",
    "# #         ## \n",
    "# #         if df is None:\n",
    "# #             df = self.df_us.copy()\n",
    "            \n",
    "            \n",
    "# #         ## Get state_df group\n",
    "# #         state_df = df.groupby(group_col).get_group(state_name)#.resample(freq).agg(agg)\n",
    "\n",
    "# #         ## Resample and aggregate state data\n",
    "# #         state_df = state_df.resample(freq).agg(agg_func)\n",
    "\n",
    "\n",
    "# #         ## Get and Rename Sum Cols \n",
    "# #         orig_cols = state_df.columns\n",
    "\n",
    "# #         ## Create Renamed Sum columns\n",
    "# #         for col in orig_cols:\n",
    "# #             state_df[f\"{state_name} - {col}\"] = state_df[col]\n",
    "\n",
    "# #         ## Drop original cols\n",
    "# #         state_df.drop(orig_cols,axis=1,inplace=True)\n",
    "\n",
    "# #         if ts_col is not None:\n",
    "# #             ts_cols_selected = [col for col in state_df.columns if ts_col in col]\n",
    "# #             state_df = state_df[ts_cols_selected]\n",
    "\n",
    "# #         return state_df \n",
    "\n",
    "    \n",
    "#     def _make_state_dict(self,df=None,col='state'):\n",
    "#         if df is None:\n",
    "#             df = self.df_us.copy()\n",
    "            \n",
    "#         elif col not in df.columns:\n",
    "#             msg = f\"{col} not in df.columns.\\nColumns include:\"+'\\n'.join(df.columns)\n",
    "#             raise Exception(msg)\n",
    "            \n",
    "#         state_list=df[col].unique()\n",
    "\n",
    "#         STATES = {}\n",
    "#         for state in state_list:\n",
    "#             STATES[state] = self.get_group_ts(state,df=df)\n",
    "#         self.STATES = STATES\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:16.983685Z",
     "start_time": "2020-07-04T01:16:14.986161Z"
    },
    "code_folding": [],
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] DOWNLOADING DATA USING KAGGLE API\n",
      "\thttps://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\n",
      "\t- Downloaded dataset .zip and extracted to:\"New Data/\"\n",
      "\t- Extraction Complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Province/State  Country/Region  Confirmed  Deaths  Recovered\n",
       "0 2020-01-22          Anhui  Mainland China        1.0     0.0        0.0\n",
       "1 2020-01-22        Beijing  Mainland China       14.0     0.0        0.0\n",
       "2 2020-01-22      Chongqing  Mainland China        6.0     0.0        0.0\n",
       "3 2020-01-22         Fujian  Mainland China        1.0     0.0        0.0\n",
       "4 2020-01-22          Gansu  Mainland China        0.0     0.0        0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] There are 223 countries in the datatset\n",
      "[i] Dates Covered:\n",
      "\tFrom 01-22-2020 to 06-30-2020\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Anhui</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Chongqing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Fujian</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Gansu</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53922</th>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>Zacatecas</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>908.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>626.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53923</th>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>Zakarpattia Oblast</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>2889.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53924</th>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>Zaporizhia Oblast</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>572.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>418.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53925</th>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>Zhejiang</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53926</th>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>Zhytomyr Oblast</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>746.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53927 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date      Province/State  Country/Region  Confirmed  Deaths  \\\n",
       "0     2020-01-22               Anhui  Mainland China        1.0     0.0   \n",
       "1     2020-01-22             Beijing  Mainland China       14.0     0.0   \n",
       "2     2020-01-22           Chongqing  Mainland China        6.0     0.0   \n",
       "3     2020-01-22              Fujian  Mainland China        1.0     0.0   \n",
       "4     2020-01-22               Gansu  Mainland China        0.0     0.0   \n",
       "...          ...                 ...             ...        ...     ...   \n",
       "53922 2020-06-30           Zacatecas          Mexico      908.0    96.0   \n",
       "53923 2020-06-30  Zakarpattia Oblast         Ukraine     2889.0    91.0   \n",
       "53924 2020-06-30   Zaporizhia Oblast         Ukraine      572.0    17.0   \n",
       "53925 2020-06-30            Zhejiang  Mainland China     1269.0     1.0   \n",
       "53926 2020-06-30     Zhytomyr Oblast         Ukraine     1404.0    29.0   \n",
       "\n",
       "       Recovered  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "53922      626.0  \n",
       "53923      943.0  \n",
       "53924      418.0  \n",
       "53925     1267.0  \n",
       "53926      746.0  \n",
       "\n",
       "[53927 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona = CoronaData(verbose=True,run_workflow=True)\n",
    "# print(corona)\n",
    "\n",
    "corona.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:26.873729Z",
     "start_time": "2020-07-04T01:16:26.871628Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip freeze > requirements-full.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T01:16:27.098529Z",
     "start_time": "2020-07-04T01:16:27.091162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "------------------------------------------------------------\n",
       "[i] CovidTrackingProject Contents:\n",
       "------------------------------------------------------------\n",
       "\n",
       "METHODS:\n",
       "\tcalculate_per_capita\n",
       "\tdownload_coronavirus_data\n",
       "\tget_and_clean_US\n",
       "\tget_data_fpath\n",
       "\tget_group_ts\n",
       "\tload_raw_df\n",
       "\tload_us_reference_info\n",
       "\tset_datetime_index\n",
       "\n",
       "ATTRIBUTES\n",
       "\tSTATES\n",
       "\tdf\n",
       "\tdf_us\n",
       "\traw_df\n",
       "\treference_data"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.020200Z",
     "start_time": "2020-07-03T19:13:32.018349Z"
    }
   },
   "outputs": [],
   "source": [
    "# corona.get_and_clean_US()\n",
    "# corona.df_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.041144Z",
     "start_time": "2020-07-03T19:13:32.021765Z"
    }
   },
   "outputs": [],
   "source": [
    "corona.df_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.044499Z",
     "start_time": "2020-07-03T19:13:32.042515Z"
    }
   },
   "outputs": [],
   "source": [
    "# corona.US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.058252Z",
     "start_time": "2020-07-03T19:13:32.046058Z"
    }
   },
   "outputs": [],
   "source": [
    "corona.df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.072442Z",
     "start_time": "2020-07-03T19:13:32.059741Z"
    }
   },
   "outputs": [],
   "source": [
    "corona.df_us.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.088738Z",
     "start_time": "2020-07-03T19:13:32.073746Z"
    }
   },
   "outputs": [],
   "source": [
    "md = corona.get_group_ts('MD')\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.133137Z",
     "start_time": "2020-07-03T19:13:32.091215Z"
    }
   },
   "outputs": [],
   "source": [
    "df = corona.df_us.copy()\n",
    "\n",
    "## Report Total Cases\n",
    "total_cases = df.groupby('state').sum()[['Confirmed','Deaths']]\n",
    "total_cases.sort_values('Confirmed',0,0).head(20).style.bar(['Deaths','Confirmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install/Import plotly,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.137833Z",
     "start_time": "2020-07-03T19:13:32.135155Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install plotly-geo==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.253095Z",
     "start_time": "2020-07-03T19:13:32.139882Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(sharing='public',theme='solar',offline=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot selected states and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.257237Z",
     "start_time": "2020-07-03T19:13:32.254584Z"
    }
   },
   "outputs": [],
   "source": [
    "STATES = corona.STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.267451Z",
     "start_time": "2020-07-03T19:13:32.258730Z"
    }
   },
   "outputs": [],
   "source": [
    "### Define states and columns to plot\n",
    "plot_states = ['NY','MD','CA']\n",
    "plot_cols = ['Confirmed']\n",
    "\n",
    "## Make empty list for states to concat\n",
    "concat_dfs = []\n",
    "\n",
    "# Grab each state's df\n",
    "for state in plot_states:\n",
    "    dfs = STATES[state]\n",
    "    \n",
    "    ## for each plot_cols, find all columns that contain that col name\n",
    "    for plot_col in plot_cols:\n",
    "        concat_dfs.append(dfs[[col for col in dfs.columns if plot_col in col]])\n",
    "\n",
    "        \n",
    "## Concatenate final dfs\n",
    "plot_df = pd.concat(concat_dfs,axis=1)#[STATES[s] for s in plot_states],axis=1).iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.722191Z",
     "start_time": "2020-07-03T19:13:32.268696Z"
    }
   },
   "outputs": [],
   "source": [
    "## Plot concatenated dfs\n",
    "pfig = plot_df.iplot()#theme='solar',asFigure=True)\n",
    "pfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.729218Z",
     "start_time": "2020-07-03T19:13:32.727020Z"
    }
   },
   "outputs": [],
   "source": [
    "# help(cf.themes.THEMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.733812Z",
     "start_time": "2020-07-03T19:13:32.731791Z"
    }
   },
   "outputs": [],
   "source": [
    "# pio.templates['plotly_dark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.741054Z",
     "start_time": "2020-07-03T19:13:32.735191Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“º DASHBOARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T01:14:48.814121Z",
     "start_time": "2020-04-05T01:14:48.808571Z"
    }
   },
   "source": [
    "### `def get_state_ts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.748122Z",
     "start_time": "2020-07-03T19:13:32.742527Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_state_ts(df, state_name,\n",
    "                     group_col='state', ts_col=None,\n",
    "                     freq='D', agg_func='sum'):\n",
    "    \"\"\"Take df_us and extracts state's data as then Freq/Aggregation provided\"\"\"\n",
    "    \n",
    "    ## Get state_df group\n",
    "    state_df = df.groupby(group_col).get_group(state_name)#.resample(freq).agg(agg)\n",
    "    \n",
    "    ## Resample and aggregate state data\n",
    "    state_df = state_df.resample(freq).agg(agg_func)\n",
    "    \n",
    "    \n",
    "    ## Get and Rename Sum Cols \n",
    "    orig_cols = state_df.columns\n",
    "\n",
    "    ## Create Renamed Sum columns\n",
    "    for col in orig_cols:\n",
    "        state_df[f\"{state_name} - {col}\"] = state_df[col]\n",
    "      \n",
    "    ## Drop original cols\n",
    "    state_df.drop(orig_cols,axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    ## Select columns from ts_cols\n",
    "    if ts_col is not None:\n",
    "        ts_cols_selected = [col for col in state_df.columns if ts_col in col]\n",
    "        state_df = state_df[ts_cols_selected]\n",
    "\n",
    "    return state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:32.752038Z",
     "start_time": "2020-07-03T19:13:32.749770Z"
    }
   },
   "outputs": [],
   "source": [
    "## Variable to control if dash app is run\n",
    "RUN_APP = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DASHBOARD FEATURES**\n",
    "- Current Features:\n",
    "    - Plotly Time Series Plot for US States\n",
    "    \n",
    "- TO DO Features:\n",
    "    - Plotly Map of Cases by State/Zipcode\n",
    "\n",
    "        - Time Series Plot by Zipcode\n",
    "\n",
    "**FOR PLOTTING:**\n",
    "- Need a plotly function that will allow for choices of data to display\n",
    "- Plotting Function options:\n",
    "    - Type of Cases: \n",
    "        - (confirmed, deaths,recovered)\n",
    "        - Per Capita versions of above\n",
    "    - Display New Cases or Cumulative Cases\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def `plot_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:35.240046Z",
     "start_time": "2020-07-03T19:13:32.753729Z"
    }
   },
   "outputs": [],
   "source": [
    "corona = CoronaData(download=True,verbose=False)#.get_state_ts('NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:35.361498Z",
     "start_time": "2020-07-03T19:13:35.242073Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(sharing='public',theme='solar',offline=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:35.368587Z",
     "start_time": "2020-07-03T19:13:35.363216Z"
    }
   },
   "outputs": [],
   "source": [
    "cf.themes.THEMES['solar']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def `plot_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:35.383013Z",
     "start_time": "2020-07-03T19:13:35.370452Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_states(df, state_list, plot_cols = ['Confirmed'],df_only=False,\n",
    "                new_only=False,plot_scatter=True,show=False):\n",
    "    \"\"\"Plots the plot_cols for every state in state_list.\n",
    "    Returns plotly figure\n",
    "    New as of 06/21\"\"\"\n",
    "    \n",
    "    ## Get state dataframes\n",
    "    concat_dfs = []  \n",
    "    STATES = {}\n",
    "    \n",
    "    ## Get each state\n",
    "    for state in state_list:\n",
    "\n",
    "        # Grab each state's df and save to STATES\n",
    "        dfs = get_state_ts(df,state)\n",
    "        STATES[state] = dfs\n",
    "\n",
    "        ## for each plot_cols, find all columns that contain that col name\n",
    "        for plot_col in plot_cols:\n",
    "            concat_dfs.append(dfs[[col for col in dfs.columns if col.endswith(plot_col)]])#plot_col in col]])\n",
    "\n",
    "    ## Concatenate final dfs\n",
    "    plot_df = pd.concat(concat_dfs,axis=1)#[STATES[s] for s in plot_states],axis=1).iplot()\n",
    "    \n",
    "    \n",
    "    ## Set title and df if new_only\n",
    "    if new_only:\n",
    "        plot_df = plot_df.diff()\n",
    "        title = \"Coronavirus Cases by State - New Cases\"\n",
    "    else:\n",
    "        title = 'Coronavirus Cases by State - Cumulative'\n",
    "    \n",
    "    ## Reset Indes\n",
    "    plot_df.reset_index(inplace=True)\n",
    "    \n",
    "    \n",
    "    ## Return Df or plot\n",
    "    if df_only==False:\n",
    "\n",
    "        if np.any(['per capita' in x.lower() for x in plot_cols]):\n",
    "            value_name = \"# of Cases - Per Capita\"\n",
    "        else:\n",
    "            value_name='# of Cases'\n",
    "        pfig_df_melt = plot_df.melt(id_vars=['Date'],var_name='State',\n",
    "                                    value_name=value_name)\n",
    "        \n",
    "        if plot_scatter:\n",
    "            plot_func = px.scatter\n",
    "        else:\n",
    "            plot_func = px.line\n",
    "            \n",
    "            \n",
    "        # Plot concatenated dfs\n",
    "        pfig = plot_func(pfig_df_melt,x='Date',y=value_name,color='State',\n",
    "                      width=800,height=500,title=title,template='plotly_dark')\n",
    "        \n",
    "#         pfig.update_xaxes(rangeslider_visible=True)\n",
    "\n",
    "                # Add range slider\n",
    "        pfig.update_layout(\n",
    "            xaxis=dict(\n",
    "                rangeselector=dict(\n",
    "                    buttons=list([\n",
    "                        dict(count=7,\n",
    "                             label=\"1week\",\n",
    "                             step=\"day\",\n",
    "                             stepmode=\"backward\"),\n",
    "                        dict(count=14,\n",
    "                             label=\"2weeks\",\n",
    "                             step=\"day\",\n",
    "                             stepmode=\"backward\"),\n",
    "                        dict(count=1,\n",
    "                             label=\"1m\",\n",
    "                             step=\"month\",\n",
    "                             stepmode=\"backward\"),\n",
    "                        dict(count=6,\n",
    "                             label=\"6m\",\n",
    "                             step=\"month\",\n",
    "                             stepmode=\"backward\"),\n",
    "\n",
    "                        dict(step=\"all\")\n",
    "                    ])\n",
    "                ),\n",
    "                rangeslider=dict(\n",
    "                    visible=True\n",
    "                ),\n",
    "                type=\"date\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if show:\n",
    "            pfig.show()\n",
    "            \n",
    "        return pfig\n",
    "    else:\n",
    "        return plot_df#.reset_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:35.934755Z",
     "start_time": "2020-07-03T19:13:35.385399Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using Function\n",
    "pfig = plot_states(df,['NY','MD','KY','CA','FL','MA','DC','VA'],\n",
    "                  plot_cols=['Confirmed'],plot_scatter=True)#,new_only=True)#,df_only=True)\n",
    "pfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APP - MOVED TO app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dashboard Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JupyterDash app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:41.588920Z",
     "start_time": "2020-07-03T19:13:35.937062Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## RUN FULL WORKFLOW\n",
    "corona = CoronaData(verbose=True,download=True,run_workflow=True)\n",
    "df = corona.df_us.copy()\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:41.595186Z",
     "start_time": "2020-07-03T19:13:41.590164Z"
    }
   },
   "outputs": [],
   "source": [
    "df['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:41.598456Z",
     "start_time": "2020-07-03T19:13:41.596587Z"
    }
   },
   "outputs": [],
   "source": [
    "# corona.STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:41.601805Z",
     "start_time": "2020-07-03T19:13:41.599785Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "# pio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.047371Z",
     "start_time": "2020-07-03T19:13:41.603588Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from jupyter_dash import JupyterDash\n",
    "except:\n",
    "    %conda install -c conda-forge -c plotly jupyter-dash\n",
    "    from jupyter_dash import JupyterDash\n",
    "\n",
    "## IMPORTS\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "\n",
    "def make_options(menu_choices):\n",
    "    \"\"\"Returns list of dictionary with {'label':menu_choice,'value':menu_choice}\"\"\"\n",
    "    options = []\n",
    "    for choice in menu_choices:\n",
    "        options.append({'label':choice,'value':choice})\n",
    "    return options\n",
    "\n",
    "## Make Plot Cols list for options\n",
    "stat_cols = ['Confirmed','Deaths','Recovered']\n",
    "plot_cols = []\n",
    "for column in stat_cols:\n",
    "    plot_cols.extend([col for col in df.columns if column in col])\n",
    "\n",
    "new_options = [{'label':'New Cases Only','value':1},\n",
    "{'label':'Cumulative Cases','value':0}]\n",
    "\n",
    "\n",
    "\n",
    "# Build App\n",
    "app = JupyterDash(name='corona',external_stylesheets=['assets/my_style.css'])\n",
    "\n",
    "app.layout = html.Div(\n",
    "    id='app',\n",
    "    children=[\n",
    "        html.H1(\"Coronavirus Analysis\"),\n",
    "        html.H2(\"Select Case Types and States\"),\n",
    "        html.Div(id=\"menu\",\n",
    "                 children=[\n",
    "                     html.Div(id='case_type_menu', \n",
    "                              children=[\n",
    "                                  dcc.RadioItems(id='choose_new',\n",
    "                                                 options=new_options,\n",
    "                                                 value=0),\n",
    "                                  dcc.Dropdown(id='choose_cases',multi=False,\n",
    "                                               placeholder='Select Case Type', \n",
    "                                               options=make_options(plot_cols),\n",
    "                                               value='Confirmed')]),\n",
    "                     dcc.Dropdown(id='choose_states',\n",
    "                                  multi=True,\n",
    "                                  placeholder='Select States', \n",
    "                                  options= make_options(df['state'].sort_values().unique( )),\n",
    "                                  value=['MD','NY','TX','CA','AZ'])\n",
    "                 ]),\n",
    "        dcc.Graph(id='graph')\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(Output('graph','figure'),[Input('choose_states','value'),\n",
    "                                       Input('choose_cases','value'),\n",
    "                                       Input('choose_new','value')])\n",
    "def update_output_div(states,cases,new_only):\n",
    "    if isinstance(states,list)==False:\n",
    "        states = [states]\n",
    "    if isinstance(cases,list)==False:\n",
    "        cases = [cases]\n",
    "\n",
    "    pfig = plot_states(df,states,plot_cols=cases,new_only=new_only)\n",
    "    return pfig\n",
    "\n",
    "\n",
    "if RUN_APP:\n",
    "    app.run_server(mode='external')\n",
    "else:\n",
    "    print('[!] Did not initialize Dash app since RUN_APP==False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ðŸ“•Covid Tracking Project Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.388004Z",
     "start_time": "2020-07-03T19:13:42.048720Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://covidtracking.com/api\n",
    "\n",
    "`/api/v1/states/{state}/screenshots.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.389407Z",
     "start_time": "2020-07-03T19:13:23.628Z"
    }
   },
   "outputs": [],
   "source": [
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.390444Z",
     "start_time": "2020-07-03T19:13:23.631Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get US Daily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.391325Z",
     "start_time": "2020-07-03T19:13:23.636Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import requests\n",
    "import json,urllib\n",
    "todays_date = dt.datetime.now().strftime('%m%d%Y')\n",
    "\n",
    "base_url = f\"http://covidtracking.com\"\n",
    "# state='ny'\n",
    "# url = f\"http://covidtracking.com/api/v1/states/{state}/screenshots.json\"\n",
    "us_daily_url = '/api/v1/us/daily.csv'\n",
    "states_daily_url = '/api/v1/states/daily.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ class `CovidTrackingProject`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.392276Z",
     "start_time": "2020-07-03T19:13:23.639Z"
    }
   },
   "outputs": [],
   "source": [
    "from functions import CovidTrackingProject\n",
    "\n",
    "\n",
    "# class CovidTrackingProject(BaselineData):\n",
    "#     base_url = f\"http://covidtracking.com\"\n",
    "#     data = dict()\n",
    "#     urls = dict(\n",
    "        \n",
    "#         us = base_url+'/api/v1/us/daily.csv',\n",
    "#                 states = base_url+'/api/v1/states/daily.csv',\n",
    "#                 states_metadata = base_url+\"/api/v1/states/info.csv\"\n",
    "#                )\n",
    "    \n",
    "#     ## Store good vs deprecated columns\n",
    "#     columns =  {'good':[\n",
    "#             'positive','negative','death','recovered',\n",
    "#             'hospitalizedCurrently','hospitalizedCumulative',\n",
    "#             'inIcuCurrently','inIcuCumulative',\n",
    "#             'onVentilatorCurrently','onVentilatorCumulative',\n",
    "#             \"state\", \"pending\", \"dataQualityGrade\", \n",
    "#             \"lastUpdateEt\", \"totalTestsViral\", \n",
    "#             \"positiveTestsViral\", \"negativeTestsViral\", \n",
    "#             \"positiveCasesViral\", \"fips\", \"positiveIncrease\", \n",
    "#             \"totalTestResults\", \"totalTestResultsIncrease\", \n",
    "#             \"deathIncrease\", \"hospitalizedIncrease\"],\n",
    "                  \n",
    "#                   'deprecated':[\n",
    "#                       'checkTimeEt','commercialScore',\n",
    "#                       'dateChecked','dateModified','grade',\n",
    "#                       'hash','hospitalized','negativeIncrease',\n",
    "#                       'negativeRegularScore','negativeScore',\n",
    "#                       'posNeg','positiveScore','score','total',\n",
    "#                   ]}\n",
    "# #     @property\n",
    "#     def help(self):\n",
    "#         print(\"\\n[HELP] For more information, check the api documentation:\")\n",
    "#         print(\"\\thttps://covidtracking.com/api\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def __init__(self,base_folder=\"New Data/\",\n",
    "#                  download=True,verbose=True,df='states'):\n",
    "#         self.base_folder = base_folder\n",
    "#         self.__verbose = verbose\n",
    "        \n",
    "        \n",
    "#         if download:\n",
    "            \n",
    "#             if self.__verbose:\n",
    "#                 print(f\"[i] DOWNLOADING DATASETS FROM COVID TRACKING PROJECT\")\n",
    "#                 print(\"\\thttps://covidtracking.com/data\")\n",
    "            \n",
    "#             workflow = [self.download_state_meta,\n",
    "#              self.download_us_daily,self.download_state_daily]\n",
    "            \n",
    "#             for method in workflow:\n",
    "#                 try:\n",
    "#                     method()\n",
    "#                 except:\n",
    "#                     print('ERROR')\n",
    "            \n",
    "#         else:\n",
    "#             raise Exception(\"Non-download loading not implemented yet.\")\n",
    "        \n",
    "#         ## Set .df attribute\n",
    "#         if df.lower()=='states':\n",
    "#             self._df_type = df\n",
    "#             self._df = self.STATES[self.columns['states']['good']].copy()\n",
    "#         elif df.lower()=='us':\n",
    "#             self._df = self.US.copy()\n",
    "        \n",
    "        \n",
    "    \n",
    "#     def get_csv_save_load(self,url, fpath,read_kws={'parse_dates':['date']}):\n",
    "#         import pandas as pd\n",
    "#         import requests\n",
    "#         response = requests.get(url).content\n",
    "        \n",
    "#         with open(fpath,'wb') as file:\n",
    "#             file.write(response)\n",
    "\n",
    "#         state_meta = pd.read_csv(fpath,**read_kws)\n",
    "#         if self.__verbose:\n",
    "#             print(f'\\t- File saved as: \"{fpath}\"')\n",
    "\n",
    "#         return state_meta\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def download_us_daily(self):\n",
    "#         key = 'us'\n",
    "#         data = self._download_data_key(key)\n",
    "# #         setattr(self,key,data)\n",
    "#         return data\n",
    "        \n",
    "        \n",
    "#     def download_state_daily(self):\n",
    "#         key = 'states'\n",
    "#         data = self._download_data_key(key)#,read_kws={})\n",
    "# #         setattr(self,key,data)\n",
    "#         return data\n",
    "    \n",
    "#     def download_state_meta(self):\n",
    "        \n",
    "#         key = 'states_metadata'\n",
    "#         data = self._download_data_key(key,read_kws={})\n",
    "        \n",
    "#         return data\n",
    "         \n",
    "\n",
    "    \n",
    "#     def _download_data_key(self,key,read_kws={'parse_dates':['date'],\n",
    "#                                              'index_col':'date'}):\n",
    "#         #Fetch the corresponding url from self.urls\"\n",
    "#         url = self.urls[key]\n",
    "        \n",
    "#         ## Get and load csv\n",
    "#         data = self.get_csv_save_load(url,fpath=self.base_folder+key+'.csv',\n",
    "#                                       read_kws=read_kws)\n",
    "#         ## Save to data dictionary\n",
    "#         self.data[key] = data.copy()\n",
    "        \n",
    "#         setattr(self,key.upper(),data)\n",
    "\n",
    "#         return data\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.393126Z",
     "start_time": "2020-07-03T19:13:23.643Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "covid=CovidTrackingProject(download=True,verbose=True)\n",
    "covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.393801Z",
     "start_time": "2020-07-03T19:13:23.646Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "covid.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.395117Z",
     "start_time": "2020-07-03T19:13:23.650Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.396411Z",
     "start_time": "2020-07-03T19:13:23.655Z"
    }
   },
   "outputs": [],
   "source": [
    "# for col in covid.states_daily.columns:\n",
    "#     if col not in  covid.columns['states']['deprecated']:\n",
    "\n",
    "#         if col not in covid.columns['states']['good']:\n",
    "#             print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.397172Z",
     "start_time": "2020-07-03T19:13:23.659Z"
    }
   },
   "outputs": [],
   "source": [
    "covid.columns['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.398260Z",
     "start_time": "2020-07-03T19:13:23.663Z"
    }
   },
   "outputs": [],
   "source": [
    "covid.STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.399044Z",
     "start_time": "2020-07-03T19:13:23.666Z"
    }
   },
   "outputs": [],
   "source": [
    "#deprecated columns according to https://covidtracking.com/data/download\n",
    "dep_cols_states = ['checkTimeEt','commercialScore',\n",
    "                  'dateChecked','dateModified','grade',\n",
    "                  'hash','hospitalized','negativeIncrease',\n",
    "                  'negativeRegularScore','negativeScore',\n",
    "                  'posNeg','positiveScore','score','total',\n",
    "                  ]\n",
    "covid.STATES[dep_cols_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [07/02/20] Looking Up FIPS/Counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.399769Z",
     "start_time": "2020-07-03T19:13:23.670Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U us\n",
    "# import us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.400848Z",
     "start_time": "2020-07-03T19:13:23.674Z"
    }
   },
   "outputs": [],
   "source": [
    "df = covid.STATES\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.402197Z",
     "start_time": "2020-07-03T19:13:23.677Z"
    }
   },
   "outputs": [],
   "source": [
    "ny = df.groupby('state').get_group('NY')\n",
    "ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.403348Z",
     "start_time": "2020-07-03T19:13:23.680Z"
    }
   },
   "outputs": [],
   "source": [
    "ny['fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.404324Z",
     "start_time": "2020-07-03T19:13:23.683Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load in Fips Data\n",
    "fips = pd.read_csv('Reference Data/ZIP-COUNTY-FIPS_2018-03.csv')\n",
    "fips.groupby('STATE').get_group(\"NY\")['STCOUNTYFP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.405367Z",
     "start_time": "2020-07-03T19:13:23.686Z"
    }
   },
   "outputs": [],
   "source": [
    "fips.loc[fips['STCOUNTYFP']==36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.406454Z",
     "start_time": "2020-07-03T19:13:23.689Z"
    }
   },
   "outputs": [],
   "source": [
    "import fsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.407480Z",
     "start_time": "2020-07-03T19:13:23.693Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = covid.STATES\n",
    "df['fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.408459Z",
     "start_time": "2020-07-03T19:13:23.697Z"
    }
   },
   "outputs": [],
   "source": [
    "# #     def __init__(self):\n",
    "# tracking = CovidTrackingProject()\n",
    "# states_daily = tracking.download_state_daily()\n",
    "# us_daily=tracking.download_us_daily()\n",
    "# state_meta = tracking.download_state_meta()\n",
    "# display(states_daily.head(),us_daily.head(),state_meta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.409497Z",
     "start_time": "2020-07-03T19:13:23.700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "covid = CovidTrackingProject(download=True)\n",
    "state_meta = covid.data['states_metadata']\n",
    "states_daily = covid.data['states']\n",
    "state_list = state_meta['state'].unique()\n",
    "states_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.410518Z",
     "start_time": "2020-07-03T19:13:23.703Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.411457Z",
     "start_time": "2020-07-03T19:13:23.707Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report  = ProfileReport(states_daily)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES: COLUMNS TO PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic Stats:\n",
    "    - death: cumulative total people died\n",
    "    - positive: total number people positive so far\n",
    "    - negative\n",
    "    - recovered\n",
    "    \n",
    "\n",
    "- Hospitalization:\n",
    "    - hospitalizedCumulative: total number hospital so far(recovered and dead)\n",
    "    - hospitalizedCurrently: \n",
    "    - hospitalizedIncrease\n",
    "\n",
    "\n",
    "- ICU:\n",
    "    - inIcuCumulative: total number hospital so far(recovered and dead)\n",
    "    - inIcuCurrently: \n",
    "    \n",
    "- Ventilator \n",
    "    - onVentilatorCumulative\n",
    "    - onVentilatorCurrently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.412414Z",
     "start_time": "2020-07-03T19:13:23.712Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "covid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.413552Z",
     "start_time": "2020-07-03T19:13:23.715Z"
    }
   },
   "outputs": [],
   "source": [
    "NY = states_daily.groupby('state').get_group('NY')[covid.columns['good']]\n",
    "NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—ºAdding Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T02:06:08.498706Z",
     "start_time": "2020-07-02T02:06:08.496433Z"
    }
   },
   "source": [
    "## Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.414673Z",
     "start_time": "2020-07-03T19:13:23.721Z"
    }
   },
   "outputs": [],
   "source": [
    "df = corona.df_us\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.415654Z",
     "start_time": "2020-07-03T19:13:23.725Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install geopandas\n",
    "# !pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.416636Z",
     "start_time": "2020-07-03T19:13:23.729Z"
    }
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "locator = Nominatim(user_agent=\"myGeocoder\")\n",
    "res = locator.geocode('Baltimore')\n",
    "res.latitude,res.longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T02:06:25.776409Z",
     "start_time": "2020-07-02T02:06:25.774464Z"
    }
   },
   "source": [
    "## Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:13:42.417632Z",
     "start_time": "2020-07-03T19:13:23.733Z"
    }
   },
   "outputs": [],
   "source": [
    "# import folium\n",
    "# center = (res.latitude,res.longitude) #(resp['region']['center']['latitude'],resp['region']['center']['longitude'])\n",
    "\n",
    "# popup = folium.Popup(f\"Latitude={center[0]}, Longitude={center[1]}\")\n",
    "# marker = folium.Marker(center,popup)\n",
    "# mymap = folium.Map(center)\n",
    "# marker.add_to(mymap)\n",
    "# mymap"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.719px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
